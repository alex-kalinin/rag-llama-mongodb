{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"275.992px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7468084,"sourceType":"datasetVersion","datasetId":4347140},{"sourceId":4302,"sourceType":"modelInstanceVersion","modelInstanceId":3097}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Prerequisite**\n1. Download LLama Model locally\n  1. https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/tree/main\n2. Preload Sentence Transformer model (run the preload code below)\n\n**Plan**\n\n1. Use PDF document (e.g. a financial report)\n2. Split using SentenceTransformer\n3. Load to MongoDB\n4. Search \n5. Add a prompt\n6. Generate","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:57:49.366650Z","iopub.execute_input":"2024-04-20T05:57:49.367475Z","iopub.status.idle":"2024-04-20T05:57:49.383322Z","shell.execute_reply.started":"2024-04-20T05:57:49.367437Z","shell.execute_reply":"2024-04-20T05:57:49.381922Z"}}},{"cell_type":"code","source":"!pip install langchain==0.1.3\n!pip install sentence-transformers==2.2.2\n!pip install \"pymongo[srv]\"\n!pip install typing-inspect==0.8.0 typing_extensions==4.5.0\n!pip install pypdf==3.17.4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.25 --force-reinstall --upgrade --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /opt/conda/lib/python3.10/site-packages/numpy-1.26.3.dist-info\n!rm -rf /opt/conda/lib/python3.10/site-packages/numpy-1.26.4.dist-info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install --force-reinstall --no-deps numpy==1.26.3\n!pip install -U  numpy==1.24.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"from pymongo import MongoClient\nimport os\nfrom llama_cpp import Llama\nfrom langchain_community.llms import LlamaCpp\nimport torch\n\n# https://www.sbert.net/docs/pretrained_models.html#model-overview\n# Sentence BERT, based on BERT\nfrom sentence_transformers import SentenceTransformer\n\n# https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.ht\n# https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.SentenceTransformersTokenTextSplitter.html\nfrom langchain.text_splitter import (\n    RecursiveCharacterTextSplitter, \n    SentenceTransformersTokenTextSplitter\n)\nfrom pypdf import PdfReader\n\nimport ctypes\nfrom llama_cpp import llama_log_set\ndef my_log_callback(level, message, user_data):\n    pass\n\nlog_callback = ctypes.CFUNCTYPE(None, ctypes.c_int, ctypes.c_char_p, ctypes.c_void_p)(my_log_callback)\nllama_log_set(log_callback, ctypes.c_void_p())\n\n# We will keep all global variables in an object to not pullute the global namespace.\nclass Object(object):\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = Object()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '') != ''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MongoDB Config","metadata":{}},{"cell_type":"code","source":"if KAGGLE:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    t.uri = user_secrets.get_secret(\"MONGODB_URI\")\nelse:\n    t.uri = os.environ[\"MONGODB_URI\"]\n# Create a new client and connect to the server\nt.client = MongoClient(t.uri)\n# Send a ping to confirm a successful connection\ntry:\n    t.client.admin.command('ping')\n    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\nexcept Exception as e:\n    print(e)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t.db = t.client.rag_llama\nt.coll = t.db.mdb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preload():\n    s = SentenceTransformersTokenTextSplitter()\n    emb = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if KAGGLE:\n    !wget https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q6_K.gguf    \n    preload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Llama Config","metadata":{}},{"cell_type":"code","source":"# t.model_path = \"../../data\"\nif KAGGLE:\n    t.llm_path = \"/kaggle/working/llama-2-13b-chat.Q6_K.gguf\"\n    t.layers = 50\nelse:    \n    t.model_path = \"../../../../data\"\n    t.llm_path = f\"{t.model_path}/llama/llama-2-13b-chat.Q6_K.gguf\"\n    t.layers = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load and Parse Documents","metadata":{}},{"cell_type":"code","source":"# t.reader = PdfReader(\"data/brk-2023-q3.pdf\")\n# t.reader = PdfReader(\"data/msft-2022.pdf\")\nif KAGGLE:\n    t.reader = PdfReader(f\"../input/mdb-pdf/{t.coll.name}-2022.pdf\")\nelse:\n    t.reader = PdfReader(f\"data/{t.coll.name}-2022.pdf\")\nt.pages = [p.extract_text().strip() for p in t.reader.pages]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pages are of various sizes. We need to split into chunks that fit into the model window, specifically, the BERT embedding 256-token sized window. \n\nSo we'll join all pages, and use the SentenceTransformer splitter to split the doc into the chunks of the right size.","metadata":{}},{"cell_type":"code","source":"# print(t.pages[10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t.ch_splitter =  RecursiveCharacterTextSplitter(\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n    chunk_size=1024,\n    chunk_overlap=0\n)\nt.ch_chunks = t.ch_splitter.split_text(\"\\n\".join(t.pages))\nlen(t.ch_chunks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t.token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=10, tokens_per_chunk=256)\nt.token_chunks = []\nfor ch in t.ch_chunks:\n    t.token_chunks.extend(t.token_splitter.split_text(ch))\nlen(t.token_chunks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding Model","metadata":{}},{"cell_type":"code","source":"t.emb_model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(t.emb_model.encode(t.token_chunks[21]).tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Upload documents","metadata":{}},{"cell_type":"code","source":"len(list(t.coll.find().limit(10)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# _ = t.coll.insert_many(t.docs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(list(t.coll.find().limit(10)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Query Index","metadata":{}},{"cell_type":"markdown","source":"Index definition:\n\n```\n{\n  \"fields\": [\n    {\n      \"type\": \"vector\",\n      \"path\": \"emb\",\n      \"numDimensions\": 768,\n      \"similarity\": \"dotProduct\"\n    }\n  ]\n}\n```","metadata":{}},{"cell_type":"code","source":"t.query = \"What was the total revenue?\"\n\nt.results = t.coll.aggregate([{\n    \"$vectorSearch\": {\n        \"queryVector\": t.emb_model.encode(t.query).tolist(),\n        \"path\": \"embedding\",\n        \"numCandidates\": 100,\n        \"limit\": 8,\n        \"index\": f\"{t.coll.name}_vector_index\"\n    }}])\n\nt.context = \"\\n\\n\".join([d['text'] for d in t.results])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(t.context[0:1000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load LLama","metadata":{}},{"cell_type":"code","source":"from langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\n# https://python.langchain.com/docs/guides/local_llms\nt.llm = LlamaCpp(\n    model_path=t.llm_path,\n    n_gpu_layers=t.layers,\n    n_threads=2, \n    n_ctx=4096, \n    n_batch=512,\n    verbose=False,\n    f16_kv=True,\n    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Query LLaMa","metadata":{}},{"cell_type":"code","source":"def ask(prompt, temp=0.8, top_p=0.95):\n    out = t.llm.invoke(\n        prompt, \n        max_tokens=512, \n        stop=[\"Q:\"], \n        temperature=temp,\n        top_p=top_p,\n        top_k=10,\n        repeat_penalty=1.2,\n    )\n    return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prompt Format:\n```\n<s>[INST] <<SYS>>\n{{ system_prompt }}\n<</SYS>>\n\n{{ user_message }} [/INST]\n```","metadata":{}},{"cell_type":"markdown","source":"### Query with RAG","metadata":{}},{"cell_type":"code","source":"def ask_with_context(question, context):\n    full_prompt = (\n    \"<s>[INST]<<SYS>>\\n\"\n    + \"You are a helpful expert financial research assistant.\" \n    + \"You answer questions about about information contained in a financial report.\"\n    + \"You will be given the user's question, and the relevant informaton from \" \n    + \"the financial report. Answer the question using only this information\" \n    + \"\\n<</SYS>>\\n\\n\"\n    + \"Information: {context}\\n\"\n    + \"Question: {question}\\n\"\n    + \"Answer:\\n\"\n    + \"[/INST]\"\n    )\n    full_prompt = full_prompt.replace(\"{context}\", context)\n    full_prompt = full_prompt.replace(\"{question}\", question)\n    ask(full_prompt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_context(question):\n    results = t.coll.aggregate([{\n    \"$vectorSearch\": {\n        \"queryVector\": t.emb_model.encode(question).tolist(),\n        \"path\": \"embedding\",\n        \"numCandidates\": 200,\n        \"limit\": 8,\n        \"index\": f\"{t.coll.name}_vector_index\"\n    }}])\n    result_texts = [d['text'] for d in results]\n    assert len(result_texts) > 0\n    context = \"\\n\\n\".join(result_texts)\n    return context","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ask_with_rag(question):\n    context = find_context(question)\n    ask_with_context(question, context)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nask_with_rag(\"What was the total revenue?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nask_with_rag(\"What was the operating income or loss?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nask_with_rag(\"What was the operating income or loss in year 2022?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nask_with_rag(\"Compare the total revenue between the years 2023 and 2022\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nask_with_rag(\"What time period does the report cover?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nask_with_rag(\"Were there any changes to the executive team?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ask_with_context(\n    \"Were there any changes to the executive team?\", \n    find_context(\"Were there any changes to the executive team?\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Query Embedded Knowledge","metadata":{}},{"cell_type":"code","source":"def ask_llm(question):\n    prompt = (\n        f\"<s>[INST]<<SYS>>\\n\"\n        + f\"You are a helpful expert financial research assistant.\" \n        + f\"\\n<</SYS>>\\n\\n\"\n        + f\"Question: {question}\\n\"\n        + f\"Answer:\\n\"\n        + f\"[/INST]\"\n    )\n    ask(prompt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nask_llm(\"What was the total revenue of MongoDB in the year ended January 31, 2023?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nask_llm(\"Were there any changes to the executive team at MongoDB in the year ended January 31, 2023?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Embeddings\n","metadata":{}},{"cell_type":"code","source":"v = Object()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v.embeddings = list(t.coll.find({}, {\"embedding\": 1}))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v.emb_values = [e['embedding'] for e in v.embeddings]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def umap_embeddings(emb, umap_t, n=1e10):\n    n = min(n, len(emb))\n    umap_emb = np.empty((n, 2))\n    for i, e in enumerate(tqdm(emb)): \n        umap_emb[i] = umap_t.transform([e])\n        if i >= n - 1: break\n    return umap_emb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import umap\nimport numpy as np\nfrom tqdm import tqdm\nimport pickle\n\n# Projections are slow, so we'll cache them\nv.umap_all_emb_path = \"umap_emb_all_docs.temp.pickle\"\n\nv.umap_transform = umap.UMAP(\n    random_state=0, \n    transform_seed=0, \n    low_memory=False).fit(v.emb_values)\n\nif os.path.exists(v.umap_all_emb_path):\n    with open(v.umap_all_emb_path, \"rb\") as f:\n        v.umap_all = pickle.load(f)\nelse:\n    v.umap_all = umap_embeddings(v.emb_values, v.umap_transform, 120)\n    with open(v.umap_all_emb_path, \"wb\") as f:\n        pickle.dump(v.umap_all, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v.question = \"What was the total revenue?\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure()\nplt.scatter(v.umap_all[:, 0], v.umap_all[:, 1], s=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v.results = t.coll.aggregate([{\n    \"$vectorSearch\": {\n        \"queryVector\": t.emb_model.encode(v.question).tolist(),\n        \"path\": \"embedding\",\n        \"numCandidates\": 200,\n        \"limit\": 8,\n        \"index\": f\"{t.coll.name}_vector_index\"\n    }}]) \nv.results = list(v.results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(v.results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v.umap_query = umap_embeddings([\n    t.emb_model.encode(v.question)\n], v.umap_transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v.umap_query","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v.umap_results = umap_embeddings(\n    [d['embedding'] for d in v.results],\n    v.umap_transform\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v.umap_results.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.scatter(v.umap_all[:, 0], v.umap_all[:, 1], s=15, color=\"gray\")\nplt.scatter(v.umap_results[:, 0], v.umap_results[:, 1], s=15, color=\"blue\")\nplt.scatter(v.umap_query[:, 0], v.umap_query[:, 1], marker=\"x\", s=100, color=\"r\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LangChain","metadata":{}},{"cell_type":"markdown","source":"We'll use LangChain to tie this all together into a simple API.","metadata":{}},{"cell_type":"code","source":"# https://python.langchain.com/docs/integrations/vectorstores/mongodb_atlas\n\nfrom langchain.chains import RetrievalQA\nfrom langchain_community.vectorstores import MongoDBAtlasVectorSearch\nfrom langchain_community.embeddings import HuggingFaceEmbeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = Object()\nl.llm = t.llm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l.lang_emb = HuggingFaceEmbeddings(model_name=\"multi-qa-mpnet-base-cos-v1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check that the embeddings model returns embeddings of the correct size of 768:","metadata":{}},{"cell_type":"code","source":"len(l.lang_emb.embed_documents(['This is a test document'])[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l.vector_search = MongoDBAtlasVectorSearch(\n    t.coll, \n    l.lang_emb, \n    index_name=\"mdb_vector_index\",\n    embedding_key=\"embedding\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l.results = list(l.vector_search.max_marginal_relevance_search(\n    query=\"What was the total revenue?\",\n    k = 8,\n))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(l.results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make a Retriever Object","metadata":{}},{"cell_type":"code","source":"l.retriever = l.vector_search.as_retriever(search_kwargs={\"k\": 8})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make the end-to-end chain object","metadata":{}},{"cell_type":"code","source":"l.qa = RetrievalQA.from_chain_type(\n    llm=l.llm, \n    retriever=l.retriever)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Query LLM with LangChain","metadata":{}},{"cell_type":"code","source":"%%time\nl.qa.invoke(\"What was the total revenue?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nl.qa.invoke(\"What time period does the report cover?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}